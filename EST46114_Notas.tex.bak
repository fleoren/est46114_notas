%----------------------------------------------------------
%
\documentclass[letterpaper,titlepage,oneside,leqno,12pt]{book}%
%
%----------------------------------------------------------
% This is a sample document for the standard LaTeX Book Class
% Class options
%       --  Body text point size:
%                        10pt (default), 11pt, 12pt
%       --  Paper size:  letterpaper (8.5x11 inch, default)
%                        a4paper, a5paper, b5paper,
%                        legalpaper, executivepaper
%       --  Orientation (portrait is the default):
%                        landscape
%       --  Printside:   oneside, twoside (default)
%       --  Quality:     final(default), draft
%       --  Title page:  titlepage, notitlepage
%       --  Columns:     onecolumn (default), twocolumn
%       --  Start chapter on left:
%                        openright(no, default), openany
%       --  Equation numbering (equation numbers on right is the default):
%                        leqno
%       --  Displayed equations (centered is the default):
%                        fleqn (flush left)
%       --  Open bibliography style (closed bibliography is the default):
%                        openbib
% For instance the command
%          \documentclass[a4paper,12pt,reqno]{book}
% ensures that the paper size is a4, fonts are typeset at the size 12p
% and the equation numbers are on the right side.
%
%   --------------------------------
%               Packages
%   --------------------------------
\usepackage{amsmath}%
\usepackage{amsfonts}%
\usepackage{amssymb}%
\usepackage{graphicx}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{mathptmx}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[spanish]{babel}
\usepackage[left=3.0cm,top=3.0cm,bottom=3.0cm,right=3.0cm]{geometry}
\usepackage[pdftex]{graphicx}
\usepackage[
	bookmarks=true,         	% show bookmarks bar?
	unicode=false,          	% non-Latin characters in Acrobat�s bookmarks
	pdftoolbar=true,        	% show Acrobat�s toolbar?
	pdfmenubar=true,        	% show Acrobat�s menu?
	letterpaper,
	colorlinks=true,       		% false: boxed links; true: colored links
	allcolors=blue,          	% color of internal links (change box color with linkbordercolor)
	pdfpagemode=none,
	pdftitle={},
	pdfauthor={Juan Carlos Martinez-Ovando},
	pdfcreator={$ $Id: ACT11302_Notas.de.Clase.tex,v 1.28 2015/10/20 $ $},
	pdfsubject={Modelos de perida, Teoria de ruina},
	pdfkeywords={Inferencia frecuentista y bayesiana}
	]{hyperref}
%\usepackage[plainpages=false, colorlinks, allcolors=blue]{hyperref}
\usepackage[plainpages=false,colorlinks,citecolor=Sepia,linkcolor=OliveGreen]{hyperref}
\usepackage[latin1]{inputenc}
\usepackage[round]{natbib}

%	Continued figures
%\DeclareCaptionFormat{cont}{#1 (cont.)#2#3\par}
\DeclareGraphicsExtensions{.pdf,.jpeg,.png,.jpg}

%	Redefenining numbering for subfigures
\renewcommand\thesubfigure{\roman{subfigure}}

%----------------------------------------------------------
\newtheorem{theorem}{Teorema}
\newtheorem{acknowledgement}[theorem]{Reconocimientos}
%\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axioma}
\newtheorem{case}[theorem]{Caso}
\newtheorem{claim}[theorem]{Aseveraci\'on}
\newtheorem{conclusion}[theorem]{Conclusi\'on}
\newtheorem{condition}[theorem]{Condici\'on}
\newtheorem{conjecture}[theorem]{Conjectura}
\newtheorem{corollary}[theorem]{Corolario}
\newtheorem{criterion}[theorem]{Criterio}
\newtheorem{definition}[theorem]{Definici\'on}
\newtheorem{example}[theorem]{Ejemplo}
\newtheorem{exercise}[theorem]{Ejercicio}
\newtheorem{lemma}[theorem]{Lema}
\newtheorem{model}[theorem]{Modelo}
\newtheorem{notation}[theorem]{Notaci\'on}
\newtheorem{problem}[theorem]{Problema}
\newtheorem{proposition}[theorem]{Proposici\'on}
\newtheorem{remark}[theorem]{Nota}
\newtheorem{solution}[theorem]{Soluci\'on}
\newtheorem{summary}[theorem]{Resumen}
\newenvironment{proof}[1][Proof]{\begin{trivlist}\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
%\newenvironment{proof}[1][Proof]{\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
%\renewenvironment{case}[1][Case.]{\begin{trivlist}\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\numberwithin{equation}{chapter}

%   --------------------------------
%               Notation
%   --------------------------------
%\newcommand{\indicator}{\boldsymbol{1}}
\newcommand{\indicator}{\mathbb{I}}
\newcommand{\incy}{\Delta y}
\newcommand{\GaD}{\text{Ga}} 
\newcommand{\PaD}{\text{Pa}} 
\newcommand{\NormD}{\text{N}} 
\newcommand{\LND}{\text{LN}} 
\newcommand{\PoD}{\text{Po}} 
\newcommand{\ExpD}{\text{Exp}} 
\newcommand{\BeD}{\text{Be}} 
\newcommand{\BinD}{\text{Bin}} 
\newcommand{\BinND}{\text{BinN}} 
\newcommand{\GeoD}{\text{Geo}} 
\newcommand{\ABzeroD}{(\alpha,\beta,0)}
\newcommand{\ABoneD}{(\alpha,\beta,1)}

\renewcommand{\Pr}{\mathbb{P}} 
\newcommand{\dd}{\text{d}}
\newcommand{\Dirac}{\delta}}
\newcommand{\Expec}{\mathbb{E}} 
\newcommand{\var}{\text{var}} 
\newcommand{\riskvar}{\text{VaR}} 
\newcommand{\riskcvar}{\text{CVaR}} 
\newcommand{\riskcvvar}{\text{varCVaR}} 
\newcommand{\ctaile}{\text{CTE}} 


%----------------------------------------------------------
\begin{document}

\frontmatter
\title{	{\large \sc Instituto Tecnol\'ogico Aut\'onomo de M\'exico}\\
			\vspace{3.5cm}
			EST-46114: M\'etodos Multivariados\\
			\vspace{0.75cm}
			{\large Inferencia bayesiana en problemas $p>>n$}\\
			\vspace{1.5cm}
			{\large Notas de Clase}
		}
\author{	Juan Carlos Mart\'inez-Ovando\\
			\vspace{0.5cm}
			\textcolor{blue}{\small \tt juan.martinez.ovando@itam.mx}
		}
\date{	\textcolor{blue}{Versi\'on: \today}
		}
\maketitle
\tableofcontents

\def\dsp{\def\baselinestretch{1.65}\large\normalsize}\dsp
%\pagestyle{fancy}
\pagestyle{plain}
\pagenumbering{arabic}
\renewcommand{\thefigure}{\thechapter.\arabic{figure}}
\renewcommand{\theequation}{\thechapter.\arabic{equation}}
\renewcommand{\thetable}{\thechapter.\arabic{table}}
\renewcommand{\thealgorithm}{\thechapter.\arabic{algorithm}}

%	----------------------------
%		Ch0. Prefacio
%	----------------------------
\chapter*{Prefacio}

   Estas notas se basan en las siguientes referencias: 
   \cite{DeelstraPlantin_RiskTheoryBook}, 
	\cite{Kass_etal_ModernActuarialBook}, 
	\cite{Klugman_etal_LossModelsBook},
	\cite{Melnikov_RiskAnalysisFinance}, 
	\cite{Panjer_OperationalRisks}, 
	\cite{Shevchenko_BayesianOperationalRiskBook}, 
	\cite{SOA_IntCollectiveRisk}, 
	\cite{SOA_PracticalRiskTheory}, y
	\cite{SOA_NewAppOperationalRisk}.
   Su contenido est\'a en etapa inicial de desarrollo.


\mainmatter

%	----------------------------
%		Introducción al paradigma bayesiano
%	----------------------------
\part{Introducción al paradigma bayesiano}

%	----------------------------
%		Ch1. Paradigma bayesino
%	----------------------------
\chapter{Paradigma Bayesiano}

La estadística es el estudio de fenómenos bajo un estado de conocimiento o información incompleto. Los fundamentos teóricos de lo que en la actualidad se conoce como estadística Bayesiana tienen su origen con la publicación de un artículo del Reverendo Thomas Bayes en 1773, dos años después de su muerte. En ese trabajo, Thomas Bayes resolvió un problema de información inversa planteado por Bernoulli, que consiste en obtener información sobre réplicas independientes de variables aleatorias Bernoulli. Una década después, Laplace retomó las ideas de Bayes y desarrolló con mayor claridad lo que en la actualidad se conoce como el paradigma Bayesiano de inferencia.

Denotemos por $\mathcal{H}_{1},\mathcal{H}_{2},...,\mathcal{H}_{p}$ a una colección de proposiciones o hipótesis excluyentes y exhaustivas, y supongamos que deseamos realizar inferencias sobre éstas con base en un nivel de información denotado por $\mathcal{I}$, el cual resume nuestra percepción e información inicial respecto a estas hipótesis. Nuestro estado de información lo expresamos a través de una medida de probabilidad definida sobre el espacio de las hipótesis o proposiciones en cuestión, condicional en nuestro estado de información, que denotamos por $P(\mathcal{H}_{i}|\mathcal{I})$, y es tal que $P(\mathcal{H}_{1}|\mathcal{I})+\cdots+P(\mathcal{H}_{p}|\mathcal{I})=1$. Nuestro aprendizaje respecto a las hipótesis consiste en la actualización del conocimiento mediante la incorporación de nueva información relevante, que denotamos por $\mathcal{D}$. Por simetría tenemos la siguiente relación  
\begin{equation}
P\left( \mathcal{D}|\mathcal{I}\right) P(\mathcal{H}_{i}|\mathcal{D},%
\mathcal{I})=P\left( \mathcal{H}_{i}|\mathcal{I}\right) P\left( \mathcal{D}|%
\mathcal{H}_{i},\mathcal{I}\right) ,
\end{equation}
para $i=1,...,p$. Si $P\left( \mathcal{D}|\mathcal{I}\right) >0$, i.e. la información relevante proporcionada por el entorno real es plausible,
entonces nuestro estado de información actualizado es de la forma 
\begin{equation}
P(\mathcal{H}_{i}|\mathcal{D},\mathcal{I})=P\left( \mathcal{H}_{i}|\mathcal{I}\right) \frac{P\left( \mathcal{D}|\mathcal{H}_{i},\mathcal{I}\right) }{P\left( \mathcal{D}|\mathcal{I}\right) }.  
\label{teobayes}
\end{equation}
La relación (\ref{teobayes}) es la representación matemática del proceso de aprendizaje y es conocida como el Teorema de Bayes, aún cuando Bayes no haya sido quien lo enunció formalmente. Esta relación muestra cómo la probabilidad inicial o \textit{a priori} respecto a las hipótesis,  $P\left(\mathcal{H}_{i}|\mathcal{I}\right) $, es actualizada a la probabilidad final o \textit{a posteriori,} $P(\mathcal{H}_{i}|\mathcal{D},\mathcal{I})$, como resultado de la incorporación de nueva información $\mathcal{D}$. El Teorema de Bayes puede ser aplicado repetidamente conforme nueva información $\mathcal{D}_{1},\mathcal{D}_{2},...$ es obtenida, en cuyo caso la distribución final se convierte en la nueva información inicial para el caso siguiente, de forma que en cualquier instante la \textit{plausibilidad} de la hipótesis $\mathcal{H}_{i}$ dependerá de la evidencia total disponible. De esta forma, captura la naturaleza secuencial del proceso de aprendizaje general que usualmente efectuamos en nuestra vida cotidiana. 

Durante los años subsecuentes del siglo XVIII y del siguiente, el paradigma Bayesia\-\-no se encontró en un estado inerte debido a que esta teoría carecía de sustentabilidad teórica respecto al enfoque frecuentista de inferencia. No fue sino hasta el segundo tercio del siglo pasado en que Harold Jeffreys y Bruno de Finetti desa\-\-rrollaron y formalizaron la teoría que actualmente se encuentra vigente. Ambos fueron defensores del paradigma Bayesiano, aunque tenían visiones distintas respecto a la conceptualización e interpretación de la probabilidad. Por un lado, Jeffreys defendió una postura \textit{objetiva} sobre el tema, y por otro lado de Finetti propuso y formalizó una visión enteramente \textit{subjetiva} donde se entiende que la probabilidad mide el grado de creencia respecto al fenómeno de interés del individuo quien la expresa. En este sentido, la información inicial en la relación del (\ref{teobayes}) es necesariamente subjetiva. La interpretación objetiva e impositiva de Jeffreys, desarrollada de manera más flexible por Richard Cox, se basa en un principio de consistencia, el cual enuncia que dos individuos con el mismo nivel de información deban necesariamente reportar la misma apreciación inicial respecto a su incertidumbre de manera que las conclusiones que éstos generan necesariamente deben de ser completamente compatibles. Con el enfoque de de Finetti esta regla no necesariamente debe de cumplirse.

El enfoque Bayesiano ha evolucionado de manera sorprendente durante los años subsecuentes. Su uso nos provee de una herramienta útil de inferencia y sobre todo de predicción, que en general puede considerarse como el problema central del análisis estadístico. Una revisión detallada respecto a la evolución del paradigma Bayesiano, y en general del proceso de inferencia estadística, la podemos encontrar en \citet{Savage_FoundationsStatistics} y \citet{Lindley_StatisticalParadox}, entre otros. En las siguientes subsecciones describiremos los principios fundamentales de inferencia estadística Bayesiana y predicción.

\section{Inferencia Estadística y el Proceso de Aprendizaje}

Supongamos que una variable aleatoria de interés, denotada por $Y$, tiene una distribución de probabilidad en la familia $\mathcal{P}=\{p(y|\boldsymbol\theta):\boldsymbol\theta \in \boldsymbol\Theta \}$, donde $\boldsymbol\theta$ es un parámetro que indiza la función de probabilidad de la variable aleatoria $Y$, y $\boldsymbol\Theta$ es un espacio parametral. Desde el enfoque Bayesiano el desconocimiento sobre el valor del parámetro de interés $\boldsymbol\theta$ es manifestado mediante la asignación de una medida de probabilidad, digamos $\pi (\boldsymbol\theta )$, que representa nuestro nivel de información sobre el verdadero valor de éste. Denotemos por $\boldsymbol y$ a un conjunto de realizaciones observables de la variable $Y$, y denotemos a la distribución de probabilidad conjunta de $\boldsymbol y$ y $\boldsymbol\theta $ por $p(\boldsymbol y,\boldsymbol\theta )$. Entonces, por las leyes básicas de probabilidad, se cumplen las siguientes relaciones 
\begin{equation}
p(\boldsymbol y,\boldsymbol\theta )=p(\boldsymbol y|\boldsymbol\theta )\pi (\boldsymbol\theta )=\pi (\boldsymbol\theta |\boldsymbol y)p(\boldsymbol y)
\label{uuno}
\end{equation}
donde $p(\boldsymbol y|\boldsymbol\theta )$ es la función de probabilidad de la v.a. $\boldsymbol y$ condicional en $\boldsymbol\theta $; y $p(\boldsymbol y)$ y $\pi (\boldsymbol\theta )$ son las funciones de densidad marginales de $\boldsymbol y$ y $\boldsymbol\theta $ respectivamente.

De las ecuaciones anteriores es posible deducir que 
\begin{eqnarray}
\pi (\boldsymbol\theta |\boldsymbol y) &=&\frac{p(\boldsymbol y|\boldsymbol%
\theta )\pi (\boldsymbol\theta )}{p(\boldsymbol y)}  \nonumber \\
&\propto &p(\boldsymbol y|\boldsymbol\theta )\pi (\boldsymbol\theta )
\label{teo-bay} \\
&\propto &verosimilitud\times inicial  \nonumber
\end{eqnarray}
donde $\propto $ denota proporcionalidad en términos de $\boldsymbol\theta $; $\pi (\boldsymbol\theta |\boldsymbol y)$ es conocida como la distribución  \textit{final} (o \textit{a posteriori}) del parámetro $\boldsymbol\theta $, condicional a la informa\-\-ción muestral $\boldsymbol y$\textbf{; } $\pi (\boldsymbol\theta )$ es la distribución \textit{inicial} (o \textit{a priori}) asignada al parámetro $\boldsymbol\theta $; y $p(\boldsymbol y|\boldsymbol\theta )$ es la función de verosimilitud, vista como función de $\boldsymbol\theta $.

La distribución inicial del parámetro $\pi (\boldsymbol\theta )$ cuantifica nuestro estado de información respecto al valor desconocido del parámetro $\boldsymbol\theta $. Este conocimiento lo actualizamos, mediante la aplicación del Teorema de Bayes, con la incorporación de información adicional relevante, por ejemplo una muestra aleatoria observada $\boldsymbol y$ de la variable de interés, que proporcione evidencia sobre el verdadero parámetro $\boldsymbol\theta $. De esta forma, nuestro conocimiento actualizado sobre el parámetro $\boldsymbol\theta $ es resumido en la distribución final, o \textit{a posteriori}, $\pi (\boldsymbol\theta |\boldsymbol y)$.

Consideremos ahora que el parámetro está particionado como $\boldsymbol\theta =(\boldsymbol\theta _{1},\boldsymbol\theta _{2})$, y que sólo un subconjun\-\-to de éste, digamos $\boldsymbol\theta _{1}$, es de interés inferencial; en este caso $\boldsymbol\theta _{2}$ es conocido como parámetro de ruido. Dada una muestra $\boldsymbol y$\textbf{,} es de interés encontrar la distribución marginal final de\textbf{\ }$\boldsymbol\theta_{1} $, sin prestar atención al valor de $\boldsymbol\theta _{2}$. De esta
forma la inferencia que se realice sobre $\boldsymbol\theta _{1}$ deberá basarse en la distribución final de $\boldsymbol\theta_{1}$ condicional en $\boldsymbol y$, la cual obtenemos con el siguiente proceso de marginalización 
\begin{eqnarray*}
\pi \left( \boldsymbol\theta _{1}|\boldsymbol y\right) &=&\int \pi \left( \boldsymbol\theta _{1},\boldsymbol\theta _{2}|\boldsymbol y\right) d\boldsymbol\theta _{2} \\
&=&\int \pi \left( \boldsymbol\theta _{1}|\boldsymbol%
\theta _{2},\boldsymbol y\right) \pi \left( \boldsymbol\theta _{2}|\boldsymbol y\right) d\boldsymbol\theta _{2} \\
&=&\mathbb{E}_{\boldsymbol\Theta _{2}|\boldsymbol y}\left\{ \pi \left( \boldsymbol\theta _{1}|\boldsymbol\theta _{2},\boldsymbol y\right) \right\}
\end{eqnarray*}
donde $\boldsymbol\Theta _{2}$ es el espacio parametral de $\boldsymbol \theta _{2}$; $\pi \left(\boldsymbol\theta _{1}|\boldsymbol\theta _{2},\boldsymbol y\right) $ es la distribución final condicional de $\boldsymbol\theta _{1}$ dado $\boldsymbol\theta _{2}$ y $\boldsymbol y$\textbf{;} y $\pi \left( \boldsymbol\theta _{1}|\boldsymbol y\right) $ es la distribución final marginal de $\boldsymbol\theta _{1}$ dado $\boldsymbol y$.

La aplicación del paradigma Bayesiano nos permite establecer un procedimiento se\-\-cuen\-\-cial de actualización de la información sobre el parámetro de interés $\boldsymbol\theta $. Supon\-\-gamos que $\boldsymbol y_{1}$ es una realización de la v.a. $Y$. Aplicando el Teorema de Bayes (\ref{teo-bay}) la distribución final de $\boldsymbol\theta $ dado $\boldsymbol y_{1}$ es $\pi (\boldsymbol\theta |\boldsymbol y_{1})$. Si posteriormente se tiene acceso a otra realización de $Y$, denotada por $\boldsymbol y_{2}$, entonces la distribución final de $\boldsymbol\theta $ dado $\boldsymbol y_{1}$ y $\boldsymbol y_{2}$, que resume nuestro conocimiento sobre $\boldsymbol\theta $ actualizado por $\boldsymbol y_{1}$ y $\boldsymbol y_{2}$, puede expresarse como 
\begin{equation}
\pi (\boldsymbol\theta |\boldsymbol y_{1},\boldsymbol y_{2})\propto p(\boldsymbol y_{2}|\boldsymbol\theta ,\boldsymbol y_{1})\pi (\boldsymbol\theta |\boldsymbol y_{1}).  
\label{sec}
\end{equation}
De la ecuación (\ref{sec}) se puede establecer un procedimiento de aprendizaje secuencial, si se considera a $\pi (\boldsymbol\theta |\boldsymbol y_{1})$ como la nueva distribución inicial para $\boldsymbol\theta $, antes de observar $\boldsymbol y_{2}$.

\section{Predicción}

Uno de los objetivos centrales del análisis estadístico es el de predecir valores futuros de una variable aleatoria de interés $Y$ condicional en la información histórica observada de la misma variable, $\boldsymbol y$, y posiblemente bajo las consideraciones de algunos otros elementos o factores adicionales relevantes. Usando el enfoque Bayesiano, los resultados siguiendo este objetivo, se resumen a través de una distribución de pro\-\-babilidad definida sobre el espacio de las variables futuras, i.e. toda la información relevante sobre la variable futura, denotada por $Y_{f}$, estará resumida en $p(y_{f}|\boldsymbol y)$, cuyo cálculo se obtiene de manera directa usando en Teorema de Bayes y un proceso simple de marginalización. De la ecuación (\ref{uuno}) tenemos que  
\[ 
p(y_{f},\boldsymbol\theta |\boldsymbol y)=p(y_{f}|\boldsymbol\theta ,
\boldsymbol y)\pi (\boldsymbol\theta |\boldsymbol y),
\] 
donde $p(y_{f}|\boldsymbol\theta ,\boldsymbol y)$ es la densidad de la variable $Y_{f}$ condicional en $\boldsymbol\theta $ y $\boldsymbol y$; y $\pi (\boldsymbol\theta |\boldsymbol y)$ 
es la distribución final de $\boldsymbol\theta $ dado $\boldsymbol y$. De esta forma, la distribución predictiva final la podemos calcular como 
\begin{eqnarray*}
p(y_{f}|\boldsymbol y) &=&\int p(y_{f},%
\boldsymbol\theta |\boldsymbol y)d\boldsymbol\theta \\
&=&\int p(y_{f}|\boldsymbol\theta ,\boldsymbol y)\pi
(\boldsymbol\theta |\boldsymbol y)d\boldsymbol\theta \\
&=&\mathbb{E}_{\boldsymbol\Theta |\boldsymbol y}\left\{ p(y_{f}|\boldsymbol%
\theta ,\boldsymbol y)\right\} .
\end{eqnarray*}
En el caso de variables aleatorias intercambiables, i.e. cuando $Y_{1},...,Y_{n}$ son condicionalmente independientes dado el parámetro $\boldsymbol\theta$, el cálculo de la densidad final de $y_{f}$ se obtiene a través de
\[
p(y_{f}|\boldsymbol y)=\int p(y_{f}|\boldsymbol\theta )\pi (\boldsymbol\theta |\boldsymbol y)d\boldsymbol\theta, 
\]
en vista de la independencia condicional de $y_{f}$ y $\boldsymbol y$ dado $\boldsymbol\theta $. El problema de inferencia y predicción es en esencia un problema de decisión estadística bajo un ambiente de incertidumbre. En la siguiente sección describiremos brevemente los elementos que conforman un problema de decisión y la solución Bayesiana óptima en el caso de inferencia o predicción puntual e inferencia y predicción general.   

\chapter{Elementos de la Teoría de Decisión\label{TD}}

El problema estadístico de inferencia y predicción es básicamente un problema de decisión en un ambiente de incertidumbre: Un problema de decisión general está compuesto por un espacio de \textit{estados de la naturaleza}, que denotaremos por $\boldsymbol\Omega $. En este espacio está definido el elemento sobre el cual reside nuestra incertidumbre y sobre el cual no tenemos ningún control. 
El espacio donde tenemos un control directo define nuestras diferentes alternativas o cursos de acción respecto al fenómeno o variable que nos interesa, y básicamente representa nuestras opciones disponibles en la búsqueda de un objetivo. Este espacio lo denotamos por $\mathcal{A}$. Cada trayectoria de decisión está compuesta por la pareja $(a,\omega )$ donde $a\in \mathcal{A}$ es la acción o postura que hemos asumido respecto a la cantidad que nos interesa $\boldsymbol\omega \in \boldsymbol\Omega $, sobre la cual, como ya mencionamos, carecemos de control. Desde luego, las acciones tomadas nos conducirán a obtener diferentes resultados, que son desconocidos, y debemos de definir una escala de preferencias de manera que nuestras acciones sean consistentes y coherentes. Esta escala de preferencias sobre todas las posibles trayectorias de decisión la podemos definir a través una función de utilidad (o pérdida según sea el caso), inducida por nuestra relación de preferencia particular, y denotada por $u:\mathcal{A}\times \boldsymbol\Omega \rightarrow \Re _{+},$ (o $l=-u$ en el caso de una función de pérdida). 

Con el enfoque Bayesiano toda la información sobre el estado de la naturaleza, $\boldsymbol\omega$, está resumida en una medida de probabilidad $p(\cdot )$ condicional en toda la información relevante disponible al momento de la toma de decisiones. La solución Bayesiana óptima consiste en elegir la acción $a^{\ast }\in \mathcal{A}$ que maximice (minimice) la utilidad (pérdida) esperada \citep[Capítulo 2]{BernardoSmith_BayesBook}.

En las siguientes subsecciones describiremos brevemente cómo se pueden obtener soluciones Bayesianas óptimas al problema de inferencia y predicción usando esta he\-\-rramienta de toma de decisiones.

\section{Estimación y Predicción Puntual}

En algunas circunstancias, cuando la distribución de una v.a. $Y$ está caracterizada por un valor parametral $\boldsymbol\theta $ desconocido, es de interés encontrar un valor específico de $\boldsymbol\theta $, digamos  $\boldsymbol\theta ^{\ast }$, que describa convenientemente la distribución de probabilidad de la v.a. $\boldsymbol Y$.

Claramente este es un problema de toma de decisiones en un ambiente de incertidumbre, donde el espacio de \textit{estados de la naturaleza} y el espacio de acciones coinciden con el espacio parametral $\boldsymbol\Theta $. En este caso asignamos una medida de penalización a la acción de elegir un valor específico $\boldsymbol\theta ^{\ast }\in \boldsymbol\Theta $ respecto al verdadero valor de $\boldsymbol\theta \in \boldsymbol\Theta $ \citep{BernardoSmith_BayesBook}. Por su naturaleza, esta función es conocida como \textit{función de pérdida} y es denotada por $l(\boldsymbol\theta ,\boldsymbol\theta^{\ast })$, que al ser función de $\boldsymbol\theta \in \boldsymbol\Theta $ es una variable aleatoria.

Como ya mencionamos, la estrategia Bayesiana óptima consiste en elegir el valor $\boldsymbol\theta ^{\ast }$ que minimice la función de pérdida esperada respecto a la distribución final de $\boldsymbol\theta $ dados los datos $\boldsymbol y$, i.e. elegiremos $\boldsymbol\theta ^{\ast }\in \boldsymbol\Theta $ tal que 
\[
\boldsymbol\theta ^{\ast }=\arg \min \left\{ \mathbb{E}_{\boldsymbol\Theta |\boldsymbol y}\left[ l(\boldsymbol\theta ,\boldsymbol\theta ^{\ast })|\boldsymbol y\right] \right\} . 
\]

Usando, por ejemplo, la función de pérdida cuadrática, se tiene que el estimador puntual Bayesiano $\boldsymbol\theta ^{\ast }$ es la media de la distribución final de $\boldsymbol\theta $, i.e. $\boldsymbol\theta ^{\ast }=\mathbb{E}(\boldsymbol\theta |\boldsymbol y)$. Otros estimadores puntuales, como la mediana y la moda de la distribución final de $\boldsymbol\theta $, pueden obtenerse como una solución alternativa si se utilizan ciertas funciones de pérdida \citep{BernardoSmith_BayesBook}.

Si nuestro interés reside en pronosticar un valor de la v.a. $\boldsymbol Y$, con base en observaciones previas de la misma, entonces los estimadores de pronóstico Bayesiano los construiremos bajo el criterio anterior en términos de la distribución \textit{predictiva final} de $\boldsymbol Y$.

\section{Inferencia y Predicción General\label{TDG}}

Supongamos que el interés del análisis estadístico es el de inferir respecto a un \textit{estado de la naturaleza}, denotado por $\boldsymbol\omega \in \boldsymbol\Omega $, que se rige de manera aleatoria y sobre el cual nuestra información es limitada e inclusive en algunos casos faltantes. Las decisiones en este caso consisten en proporcionar alguna aseveración estadística respecto al valor incierto de $\boldsymbol\omega $, que desde un enfoque Bayesiano es resumida en una medida de probabilidad. Desde luego estas aseveraciones estarán condicionadas en la información relevante disponible al momento de la toma de decisiones, la cual en este caso denotamos por $D$, y que en términos generales está constituida por un conjunto de datos observados relacionados con el problema. En este caso el espacio de acciones estará definido por $\mathcal{A}=\left\{ p_{i}(\cdot |D):i\in I\right\} $, donde $p_{i}(\cdot |D)$ es una medida de probabilidad definida en $\boldsymbol\Omega $, para $i\in I$ con $I$ un conjunto índice. Así, el conjunto de todas las posibles trayectorias del problema de decisión estarán denotadas por el conjunto $\mathcal{C}=\left\{ c_{i}:i\in I\right\} ,$ donde $c_{i}=\{p_{i}(\cdot |D),\boldsymbol\omega \}$ para todo $\boldsymbol\omega \in \boldsymbol\Omega $. La especificación de un problema de decisión general requiere establecer una relación de preferencia que cuantifique la consecuencia de decidir por el modelo $p_{i}(\cdot |D)$ cuando el estado de la naturaleza es $\boldsymbol\omega $. 

La relación de preferencias se define en términos de una función de puntaje \citep[definición 3.15]{BernardoSmith_BayesBook}  $u:\mathcal{A}\times \Omega\rightarrow \Re $. Así, la solución Bayesiana óptima consiste en elegir la distribución (o densidad) de la clase $\mathcal{A}$ que maximice en $I$ la utilidad esperada  
\begin{equation}
\overline{u}(p_{i}(\cdot |D))=\int u(p_{i}(\cdot |D),\boldsymbol\omega )p(\boldsymbol\omega |D)d\boldsymbol\omega ,  
\label{utilidad}
\end{equation}
donde $p(\boldsymbol\omega |D)$ es la ``verdadera'' densidad de $\boldsymbol\omega $ condicional en los datos observados $D$.

Se dice que una función de puntaje es \textit{propia} si la utilidad esperada máxima se obtiene cuando $\sup_{i\in I}\overline{u}(p_{i}(\cdot |D))=\overline{u}(p(\cdot |D))$, i.e. cuando la opción óptima es la ``verdadera'' densidad (distribución) para $\boldsymbol \omega$, y la función de puntaje es \textit{local} si $u(p_{i}(\cdot|D),\boldsymbol\omega )=u(p_{i}(\boldsymbol\omega |D))$ para todo $\boldsymbol\omega\in \boldsymbol\Omega$, i.e. si depende sólo del valor de la densidad (distribución) evaluada en $\boldsymbol \omega$.

\citet{Bernardo_ExpUti} demostró que si una función de puntaje es propia y local, entonces debe ser de la forma 
\begin{equation}
u\left( p_{i}(\cdot |D),\boldsymbol\omega \right) =A\log p_{i}(\boldsymbol\omega |D)+B(\boldsymbol\omega ),  
\label{log}
\end{equation}
para todo $\boldsymbol\omega \in \boldsymbol\Omega $, con $A>0$ una constante real y $B(\cdot )$ una función integrable respecto a $p(\cdot |D).$ La función (\ref{log}) es conocida como \textit{función de puntaje logarítmico}.

\chapter{Integración de Monte Carlo}

Como vimos en las secciones anteriores, resolver un problema estadístico con el enfoque Bayesiano consiste operativamente en resolver integrales. En la práctica, muchas de estas integrales pueden ser difíciles de trabajar analíticamente. A través de la historia se han propuesto diferentes métodos para resolver algunos problemas de integración con estas características, algunos de los cuales consisten en aproximaciones numéricas determi\-\-nistas o analíticas a la integral de interés. Las aproximaciones analíticas se basan en la aproximación de Laplace y resultan particularmente útiles para el caso de modelos cuya distribución pertenece a la familia exponencial dedistribuciones. En esta sección describiremos el método de Monte Carlo, que sirve para aproximar integrales complejas mediante técnicas de simulación estocástica. Para efectos prácticos supon\-\-gamos que deseamos resolver una integral de la forma $\int g(\boldsymbol\theta )\pi (\boldsymbol\theta)d\boldsymbol\theta $, donde $\boldsymbol\theta \in \boldsymbol\Theta \subset \Re ^{p}$ es una variable aleatoria, $\pi (\cdot )$ es la densidad de $\boldsymbol\theta $, y $g(\cdot )$ es una función real conocida e integrable respecto a $\pi $. En el enfoque Bayesiano $\pi (\boldsymbol\theta )$ estará condicionada en la información relevante disponible al momento del análisis, denotada por $D$, que por simplicidad en la notación es omitida en el transcurso de esta sección. Los resultados de esta sección son aplicables en los casos en que $\boldsymbol\theta $ represente algunos parámetros asociados a un modelo, o cuando represente variables aleatorias observables.

El método de Monte Carlo se basa en el supuesto que seamos capaces de
generar una muestra de tamaño $N$, $\{ \boldsymbol\theta ^{(1)},...,
\boldsymbol\theta ^{(N)}\} $, de la distribución $\pi (\boldsymbol%
\theta )\footnote{Por simplicidad $\pi $ denotará a la distribución de $\theta $ y a su densidad de manera indistinta.}.$ Usando esta muestra podemos aproximar el valor de la integral de interés, la cual podemos interpretar como el valor esperado de $g$, 
\begin{equation}
\mathbb{E}_{\pi }\left[ g(\boldsymbol\theta )\right] =\int g(\boldsymbol\theta
)\pi (\boldsymbol\theta )d\boldsymbol\theta ,  
\label{integral}
\end{equation}
mediante el promedio empírico 
\begin{equation}
\widehat{\mathbb{E}}_{\pi }\left[ g(\boldsymbol\theta )\right] =\frac{1}{N}%
\sum_{i=1}^{N}g(\boldsymbol\theta ^{(i)}).  
\label{monte-carlo}
\end{equation}
El estimador (\ref{monte-carlo}), conocido como el estimador de Monte Carlo de (\ref{integral}), es un estimador insesgado y converge casi seguramente al valor de la integral de interés. Cuando la espe\-\-ranza de $g^{2}(\cdot )$ es finita respecto a $\pi (\cdot )$, la convergencia de (\ref{monte-carlo}) puede medirse en términos de su varianza teórica 
\begin{equation}
var\left[ \widehat{\mathbb{E}}_{\pi }\left[ g(\boldsymbol\theta )\right] \right]
=\frac{1}{N}\int \left[ g(\boldsymbol\theta )-\mathbb{E}[g(\boldsymbol\theta )]%
\right] ^{2}\pi (\boldsymbol\theta )d\boldsymbol\theta ,  
\label{varianza}
\end{equation}
la cual puede estimarse usando la misma muestra mediante su contraparte
muestral 
\begin{equation}
\widehat{var}\left[ \widehat{\mathbb{E}}_{\pi }\left[ g(\boldsymbol\theta )%
\right] \right] =\frac{1}{N^{2}}\sum_{i=1}^{N}\left[ g(\boldsymbol\theta
^{(i)})-\widehat{\mathbb{E}}_{\pi }\left[ g(\boldsymbol\theta )\right] \right]^{2}.
\end{equation}

En un contexto de inferencia Bayesiana generalmente conocemos la
densidad $\pi (\cdot )$ salvo por una constante de normalización, que
usualmente es difícil de calcular, y de hecho nos remonta al problema
inicial de resolver una integral como (\ref{integral}) con $g(\cdot )$ igual a la función constante unitaria. En este caso es difícil generar datos de la distribución $\pi (\cdot )$ directamente, y por ende es difícil aplicar el método de Monte Carlo. A través de la historia se han propuesto diferentes alternativas para generar datos de densidades conocidas salvo por una constante de normalización. Algunos de éstos los describiremos brevemente en las siguientes subsecciones.

\section{Muestreo por Importancia}

El muestreo por importancia consiste en suponer que tenemos acceso a una
densidad $p(\cdot )$ ``semejante'' a la densidad de interés $\pi (\cdot )$, conocida como \textit{función de densidad de importancia}, de la cual es relativamente simple generar datos muestrales. La idea consiste en utilizar estos datos para aproximar integrales de la forma (\ref{integral}) usando el método de Monte Carlo.

La base central de este método consiste en suponer que el soporte de $%
p(\cdot )$ contiene al soporte de la densidad de interés $\pi (\cdot )$, en cuyo caso (\ref{integral}) puede ser re-expresada como 
\[
\int g(\boldsymbol\theta )\frac{\pi (\boldsymbol\theta )}{p(\boldsymbol%
\theta )}p(\boldsymbol\theta )d\boldsymbol\theta =\mathbb{E}_{p}\left[ g(%
\boldsymbol\theta )w(\boldsymbol\theta )\right] , 
\]
donde $w(\boldsymbol\theta )=\pi (\boldsymbol\theta )/p(\boldsymbol \theta ).$ De esta forma, si podemos generar una muestra de la densidad $p(\cdot ),$ de tamaño $N$, $\{\boldsymbol\theta ^{(i)}:i=1,...,N\}$, entonces podemos aproximar (\ref{integral}) mediante 
\begin{equation}
\widehat{\mathbb{E}}_{\pi }[g(\boldsymbol\theta )]=\frac{1}{N}\sum_{i=1}^{N}
g(\boldsymbol\theta ^{(i)})w(\boldsymbol\theta ^{(i)}).  \label{mu-imp}
\end{equation}

Cuando $\pi (\cdot )$ es conocida salvo por una constante de normalización, la aproximación (\ref{mu-imp}) no puede ser usada directamente, sin embargo podemos expresar (\ref{integral}) como el cociente de dos esperanzas 
\begin{equation}
\mathbb{E}_{\pi }[g(\boldsymbol\theta )]=\frac{\int g(\boldsymbol\theta )\frac{\pi (\boldsymbol\theta )}{p(\boldsymbol\theta )}p(\boldsymbol\theta )d\boldsymbol\theta }{\int \frac{\pi (\boldsymbol\theta )}{p(\boldsymbol\theta
)}p(\boldsymbol\theta )d\boldsymbol\theta },
\end{equation}
en cuyo caso $\mathbb{E}_{\pi }[g(\boldsymbol\theta )]$ puede aproximarse como el cociente de dos aproximaciones de Monte Carlo como 
\[
\widehat{\mathbb{E}}_{\pi }[g(\boldsymbol\theta )]=\sum_{i=1}^{N}g(\boldsymbol%
\theta ^{(i)})\widetilde{w}(\boldsymbol\theta ^{(i)}), 
\]
donde $\widetilde{w}(\boldsymbol\theta ^{(i)})=w(\boldsymbol\theta
^{(i)})/\sum_{j=1}^{M}w(\boldsymbol\theta ^{(j)})$ son los pesos asociados a cada dato $\boldsymbol\theta ^{(i)}$, con\ $w(\boldsymbol\theta ^{(i)})$ definida como antes, para $i=1,...,M$. En este caso se hace evidente que no necesitamos la constante de normalización de la densidad de interés $\pi (\cdot )$.

La convergencia de (\ref{mu-imp}) a (\ref{integral}) se garantiza si
elegimos $p(\cdot )$ de manera que su soporte contenga al soporte de la
densidad de interés $\pi (\cdot )$. Una consideración adicional para tener una convergencia más rápida es que el cociente $\pi (\boldsymbol\theta)/p(\boldsymbol\theta )$ esté acotado para todos los valores de $\boldsymbol\theta $, y que adicionalmente las colas de $p(\cdot )$ sean más pesadas respecto a las colas de $\pi(\cdot )$. Una descripción detallada de este método se encuentra en \citet{RobertCasella_MCMCBook} describe algunas condiciones adicionales.

\section{Muestreo-Remuestreo por Importancia}

Este método extiende de manera natural las aproximaciones del método de
muestreo por importancia.

El algoritmo funciona en dos etapas. En la primera etapa suponemos que
tenemos una muestra de tamaño $N$ de una densidad de importancia $p(\cdot )$, al igual que en la subsección anterior, i.e. tenemos una muestra $\{\boldsymbol\theta ^{(i)}:i=1,...,N\}$ de la densidad $p(\boldsymbol\theta )$, donde cada $\boldsymbol\theta ^{(i)}$ tiene un peso asociado $\widetilde{w}(\boldsymbol\theta ^{(i)})$, definido como en la subsección anterior. La segunda etapa del algoritmo consiste en generar $N$ muestras con reemplazo de los valores $\{\boldsymbol\theta ^{(i)}:i=1,...,N\}$ de acuerdo a sus correspondientes pesos $\{\widetilde{w}(\boldsymbol\theta^{(i)}):i=1,...,N\} $. De esta forma podemos aproximar (\ref{integral}) por 
\begin{equation}
\widehat{\mathbb{E}}\left[ g(\boldsymbol\theta )\right] =\sum_{j=1}^{N}g(%
\widetilde{\boldsymbol\theta }^{(j)})\widetilde{w}(\widetilde{\boldsymbol%
\theta }^{(j)}),  \label{sis}
\end{equation}
donde $\{\widetilde{\boldsymbol\theta }^{(j)}:j=1,...,N\}$ son una muestra de la variable discreta $\widetilde{\boldsymbol\Theta }=\{\boldsymbol\theta ^{(i)}:i=1,...,N\}$, donde cada $\boldsymbol\theta ^{(i)}$ tiene asociada una masa de probabilidad $\widetilde{w}(\boldsymbol\theta ^{(i)}).$

Más aún, con este procedimiento podemos aproximar características de $\pi
(\cdot )$ que no pueden ser expresadas en forma de esperanza, como cuantiles e intervalos de cre\-\-dibilidad, ya que la distribución que asigna una masa $\widetilde{w}(\boldsymbol\theta ^{(i)})$ a $\boldsymbol\theta ^{(i)}$ en $\widetilde{\boldsymbol\Theta }$ tiende en distribución a $\pi (\boldsymbol\theta )$ cuando $N\rightarrow \infty $.

Este procedimiento es flexible y rico, en el sentido que podemos obtener
muestras aproximadas que nos permiten reconstruir a $\pi (\cdot )$, por ejemplo a través de histogramas o aproxi\-\-maciones por \textit{kernel} (vea el apéndice \ref{apendice.cap5}). Además permite implementar el Teorema de Bayes de manera directa, donde $\pi (\boldsymbol\theta |\boldsymbol y)\varpropto p(\boldsymbol y|\boldsymbol\theta )\pi (\boldsymbol y)$. Si podemos generar una muestra aleatoria $\{\boldsymbol\theta^{(i)}:i=1,...,N\}$ de $\pi (\boldsymbol\theta )$, podemos actualizarla a través de la verosimilitud para obtener una muestra $\{\widetilde{\boldsymbol\theta }^{(j)}\}$ de tamaño $N$, que se distribuya aproximadamente como $\pi(\boldsymbol\theta |\boldsymbol y)$, remuestreando de $\{\boldsymbol\theta^{(i)}:i=1,...,M\}$, donde cada $\boldsymbol\theta ^{(i)}$ tiene asociado una masa de probabilidad definida como $\widetilde{w}(\boldsymbol\theta^{(j)})=p(\boldsymbol y|\boldsymbol\theta ^{(j)})/\sum_{i=1}^{N}p(\boldsymbol y|\boldsymbol\theta ^{(i)})$ para $j=1,...,N$.

\section{Monte Carlo vía Cadenas de Markov}

Otro método importante para generar muestras de una distribución de
probabilidad $\pi (\cdot )$ de interés, es construyendo una cadena de Markov cuya distribución invariante sea nuestra distribución objetivo $\pi (\cdot )$.

Supongamos que podemos construir una cadena de Markov homogénea $(\boldsymbol\theta ^{(n)})_{n\geq 1}$ en tiempo discreto, con un espacio de estados $\boldsymbol\Theta \subset \Re ^{p}$. En un esquema general, esta cadena de Markov está determinada mediante una función $K$ : $\boldsymbol\Theta \times\mathcal{B}(\boldsymbol\Theta )$ $\rightarrow \lbrack 0,1]$ de transición de estados, conocida como \textit{kernel de transición}, donde $\mathcal{B}(\boldsymbol\Theta )$ es el $\sigma $-álgebra de Borel inducido por $\boldsymbol\Theta .$ En el caso que $\boldsymbol\Theta $ sea continuo, el \textit{kernel} de transición denota a la densidad condicional de transición, $K(\boldsymbol\theta ,\boldsymbol\theta')$, tal que $P\left(\boldsymbol\Theta \in A|\boldsymbol\theta \right) =\int_{A}K(\boldsymbol\theta ,d\boldsymbol\theta').$ Cuando el espacio de estado $\boldsymbol\Theta $ es discreto, el \textit{kernel} de transición denota la probabilidad de transición $K(\boldsymbol\theta ,\boldsymbol\theta')=P(\boldsymbol\Theta ^{(k+1)}=\boldsymbol\theta '|\boldsymbol\Theta ^{(k)}=\boldsymbol\theta )$ para todo $\boldsymbol\theta $ y $\boldsymbol\theta '\in \boldsymbol\Theta$ entre las iteraciones $k$ y $k+1$.

La idea central del método de Monte Carlo vía Cadenas de Markov (MCCM) es
que la cadena de transición definida por un \textit{kernel} de transición $K(\cdot
,\cdot )$ tenga a $\pi (\cdot ),$ la distribución de interés, como distribución \textit{invariante}\footnote{$\pi $ es la densidad invariante de la cadena de Markov definida por el \textit{kernel} $K(\cdot ,\cdot )$ si $\boldsymbol\theta ^{(k)}\thicksim \pi $ implica que $\boldsymbol\theta^{(k+1)}\thicksim \pi ,$ i.e. $\lim_{k\rightarrow \infty }K^{k}(\boldsymbol\theta ,A)=\pi (A)$, para todo $A\in \mathcal{B}(\boldsymbol\Theta).$}. Este enfoque de análisis de cadenas de Markov es inverso al enfoque tradicional, ya que debemos construir una cadena partiendo de la distribución invariante, en lugar de construir una cadena con un \textit{kernel} arbitrario y verificar si cumple con las condiciones de estabilidad. Si somos capaces de definir un \textit{kernel} de transición que satisfaga la condición de balance $K(\boldsymbol\theta ,\boldsymbol\theta')\pi (\boldsymbol\theta )=K(\boldsymbol\theta ',\boldsymbol\theta )\pi (\boldsymbol\theta ')$ para todo $\boldsymbol\theta $ y $\boldsymbol\theta '\in \boldsymbol\Theta $, entonces tenemos que la cadena de Markov construida con este \textit{kernel} tiene como densidad invariante a $\pi (\cdot )$ \citep[Teorema 6.2.2]{RobertCasella_MCMCBook}. Si la cadena es \textit{irreducible}\footnote{Una cadena de Markov es \textit{irreducible} ($\pi $-irreducible) si para todo $\boldsymbol\theta \in E\in \mathcal{B}(\boldsymbol\Theta )$ tal que $%
\pi (E)>0$ se tiene que para todo $A\in \mathcal{B}(\boldsymbol\Theta )$ con $\pi (A)>0$ existe algún entero $n$ tal que $K^{n}(\boldsymbol\theta ,A)>0$, i.e. si existe la libertad de que la cadena se mueva sobre todo el espacio de estados.} y \textit{aperiódica}\footnote{Una cadena de Markov es \textit{aperiódica} si no existe una partición $\left\{ E_{0},...,E_{d-1}\right\} $ del espacio de estados\thinspace $\boldsymbol\Theta $ tal que $K(\boldsymbol\theta ,E_{j})=1$ para todo $%
\boldsymbol\theta \in E_{j-1}$, i.e. no existe una trayectoria determinista de visitas a subconjuntos de $\boldsymbol\Theta $.}, entonces 
\citep{RobertCasella_MCMCBook}
\begin{itemize}
\item  $\boldsymbol\theta ^{(k)}\stackrel{d}{\rightarrow }\boldsymbol\theta
\backsim \pi $, y

\item  $\frac{1}{N}\sum_{k=1}^{N}g(\theta ^{(k)})\rightarrow \int g(\theta
)\pi (\theta )d\theta ,$ casi seguramente cuando $N\rightarrow \infty .$
\end{itemize}

En las siguientes subsecciones describiremos diferentes métodos para construir cadenas de Markov con estas características.

\section{Algoritmo de Metropolis-Hastings (M-H)}

Para una cadena de Markov $(\boldsymbol\theta ^{(k)})_{k\geq 1}$, elegimos una familia de densidades $q(\boldsymbol\theta ,\boldsymbol\theta ^{\prime}) $ parame\-\-trizadas por $\boldsymbol\theta $, i.e. para un valor de $\boldsymbol\theta $ fijo $q(\boldsymbol\theta ,\cdot )$ es una densidad con el mismo soporte que la densidad objetivo $\pi (\cdot ).$ La elección de esta familia es arbitraria con el requisito que la cadena definida por la densidad de transición $P(\boldsymbol\Theta ^{(k+1)}=\boldsymbol\theta'|\boldsymbol\Theta ^{(k)}=\boldsymbol\theta )=q(\boldsymbol\theta ,\boldsymbol\theta ')$ sea irreducible, y que satisfaga la condición de balance.

El algoritmo funciona de la siguiente manera. Dado un estado actual de la
cadena, digamos $\boldsymbol\Theta ^{(k)}=\boldsymbol\theta^{(k)} $, un valor $\boldsymbol\theta '$ es propuesto para el estado $\boldsymbol\Theta
^{(k+1)}$ con base en la densidad de transición $q(\boldsymbol\theta^{(k)} ,\boldsymbol\theta ')$, y es aceptado con una probabilidad 
\begin{equation}
\alpha(\boldsymbol\theta^{(k)},\boldsymbol\theta^{'}) =\min \left( 1,\frac{q(\boldsymbol\theta ',\boldsymbol\theta^{(k)}
)\pi (\boldsymbol\theta ')}{q(\boldsymbol\theta^{(k)} ,\boldsymbol\theta
')\pi (\boldsymbol\theta^{(k)} )}\right) ,  
\label{m-h}
\end{equation}
i.e. con probabilidad $\alpha $ el valor de la cadena en la iteración $k+1$ es $\boldsymbol\theta ^{(k+1)}=\boldsymbol\theta '$, de lo contrario $%
\boldsymbol\theta ^{(k+1)}=\boldsymbol\theta^{(k)} $. Este esquema de muestreo define una cadena de Markov con un \textit{kernel} de transición de la iteración $k$ a $k+1$ dada por
\[
K(\boldsymbol\theta^{(k)},d\boldsymbol\theta^{'})=q(\boldsymbol\theta^{(k)},d\boldsymbol\theta^{'})\alpha(\boldsymbol\theta^{(k)},\boldsymbol\theta^{'})+\left( 1-\int  \alpha(\boldsymbol\theta^{(k)},\boldsymbol\theta')q(\boldsymbol\theta^{(k)},d\boldsymbol\theta^{'})\right)  \delta_{\boldsymbol\theta^{(k)}}(d\boldsymbol\theta^{'}) .
\]
\citet{Tierney_MCMC} demostró que una cadena de Markov construida de esta forma es reversible y aperiódica, con lo cual se tiene que $\pi (\cdot )$ es su correspondiente distribución estacionaria. Este algoritmo es particularmente útil en el contexto de inferencia Baye\-\-sia\-\-na, donde en algunas ocasiones $\pi(\boldsymbol\theta)$ es conocida salvo su constante de normalización, pues la distribución de interés $\pi$ sólo es usada a través del cociente $\pi(\boldsymbol\theta^{'})/ \pi(\boldsymbol\theta^{(k)})$ en (\ref{m-h}).

Utilizando este esquema de muestreo es posible determinar diferentes algoritmos de actualización, dependiendo de la definición de la distribución $q$ por utilizar. El algoritmo original considera un esquema de muestreo independiente, i.e. $q(\boldsymbol\theta^{(k)},\boldsymbol\theta^{'})=q(\boldsymbol\theta^{'})$, en cuyo caso el cociente en (\ref{m-h}) se reduce al cociente $w(\boldsymbol\theta^{'})/w(\boldsymbol\theta^{(k)})$, donde $w(\boldsymbol\theta)=\pi(\boldsymbol\theta)/q(\boldsymbol\theta)$ denota los pesos de importancia definidos previamente para aproximar integrales emplean\-\-do como distribución de importancia a $q$. Otra alternativa consiste en definir $q$ como una distribución simétrica, i.e. $q(\boldsymbol\theta^{(k)},\boldsymbol\theta^{'})=q(\boldsymbol\theta^{'},\boldsymbol\theta^{(k)})$, en cuyo caso el cociente en (\ref{m-h}) se simplifica de la forma $\pi(\boldsymbol\theta^{'})/ \pi(\boldsymbol\theta^{(k)})$. Otra posibilidad consiste en definir $q(\boldsymbol\theta^{(k)},\boldsymbol\theta^{'})$ a través de la relación $\boldsymbol\Theta=\boldsymbol\theta^{(k)}+\boldsymbol Z$, donde $\boldsymbol Z$ es una variable aleatoria con media cero y función de distribución $r$. En este caso $q$ se define a través de una caminata aleatoria, de manera que el algoritmo se concentra en explorar vecindades contiguas al estado previo de la cadena en el espacio de estados de la cadena. Es deseable que la distribución $r$ sea simétrica. Para el caso de espacios no acotados, la distribución Normal (multivariada) o $t$ son dos alternativas útiles y simples.  

\section{Muestreador de Gibbs}

En algunas ocasiones tenemos el interés de obtener una muestra de una
distribución $\pi (\cdot )$ multivariada. De esta forma, para obtener una
muestra de $\pi (\cdot )$ mediante MCCM es necesario construir una cadena de Markov con un espacio de estado multivariado. En este caso el muestreador de Gibbs resulta un método práctico para construir tales cadenas preservando las características antes mencionadas.

Para estos efectos supongamos que la distribución de interés $\pi $
corresponde a una variable aleatoria $p$-dimensional $\boldsymbol\Theta $, y que por razones prácticas podemos descomponer este espacio en $q\leq p$ componentes, denotados por $\boldsymbol\Theta _{1},...,\boldsymbol\Theta_{q} $, algunos de éstos posiblemente multivariados, y denotemos por $\boldsymbol\Theta_{-l} $ a los componentes de $\boldsymbol\Theta$ menos el $l$-ésimo, para $l=1,2,...,q$. 

Dados los valores de la cadena en la iteración $k$, $\boldsymbol\theta
^{(k)}=(\boldsymbol\theta _{1}^{(k)},...,\boldsymbol\theta _{q}^{(k)})$, producimos la transición al estado $\boldsymbol\theta ^{(k+1)}$ mediante un muestreo sucesivo de las distribuciones condicionales completas mediante el siguiente esquema:
\begin{equation}
\begin{tabular}{lll}
$\boldsymbol\theta _{1}^{(k+1)}$ & $\sim $ & $\pi (\boldsymbol\theta _{1}|\boldsymbol\theta _{2}^{(k)},...,\boldsymbol\theta _{q}^{(k)})$ \\ 
$\boldsymbol\theta _{2}^{(k+1)}$ & $\sim $ & $\pi (\boldsymbol\theta _{2}|\boldsymbol\theta _{1}^{(k+1)},\boldsymbol\theta _{3}^{(k)},...,\boldsymbol\theta _{q}^{(k)})$ \\ 
& $\vdots $ &  \\ 
$\boldsymbol\theta _{q}^{(k+1)}$ & $\sim $ & $\pi (\boldsymbol\theta _{q}|\boldsymbol\theta _{1}^{(k+1)},...,\boldsymbol\theta _{q-1}^{(k+1)}).$
\end{tabular}
\label{gibbs}
\end{equation}

La estructura de actualización de los componentes dentro del algoritmo (\ref{gibbs}) puede definirse de manera aleatoria o determinista, considerando que cada componente es\ actualizado en cada ciclo al menos una vez. De esta forma la transición del estado $\boldsymbol\theta ^{(k)}$ al estado $\boldsymbol\theta ^{(k+1)}$ está determinada por: 
\[
K(\boldsymbol\theta ^{(k)},\boldsymbol\theta ^{(k+1)})=\prod_{l=1}^{q}\pi (\boldsymbol\theta _{l}^{(k+1)}|\boldsymbol\theta _{j}^{(k+1)},j<l,\boldsymbol\theta _{l}^{(k)},j>l).  
\]

En este algoritmo debe de considerarse la estructura de dependencia de los componentes individuales, y es recomendable agrupar en un bloque a aquellos componentes escalares que estén altamente correlacionados, para evitar que la cadena retarde su entrada al periodo de estabilidad. El muestreador de Gibbs puede ser visto como un caso particular del algoritmo de Metropolis-Hastings. En este caso el proceso de actualización se realiza en cada uno de los $q$ componentes de $\boldsymbol\Theta $ de la manera antes mencionada, entonces el valor propuesto de la cadena para el siguiente estado en cada $l$-ésimo componente es actualizado, con probabilidad 1, de la densidad $\pi (\boldsymbol\theta_{l}^{(k+1)}|\boldsymbol\theta_{j}^{(k+1)},j<l,\boldsymbol\theta _{l}^{(k)},j>l)$.

Para implementar el muestreador de Gibbs de manera directa es necesario conocer de manera cerrada cada uno de las distribuciones condicionales completas y tener la capacidad de muestrear datos de ellas también directamente. En algunas ocasiones no es posible obtener de manera cerrada algunas de las distribuciones condicionales completas, que se conocen salvo su constante de normalización. En este caso, es posible diseñar el muestreador de Gibbs incorporando en la etapa de muestreo de la condicional no normalizada la generación de muestras de una distribución ins\-\-trumental e incorporando ésta al proceso de muestreo mediante un paso adicional de importancia para la aceptación de esta muestra. Este algoritmo se conoce como muestreador de Gibbs por Importancia. Supongamos, sin pérdida de generalidad, que no es posible obtener la distribución condicional completa del $j$-ésimo componente en (\ref{gibbs}), y que podemos definir una distribución instrumental $q(\boldsymbol\theta_{j}|\boldsymbol\theta_{-j})$ completamente determinada que aproxima a $\pi(\boldsymbol\theta_{j}|\boldsymbol\theta_{-j})$. Esta distribución se deriva de la distribución instrumental $q(\boldsymbol\theta)$ que aproxima a la distribución final completa $\pi(\boldsymbol\theta)$. Así, en la $j$-ésima etapa de muestreo correspondiente de (\ref{gibbs}), donde el estado parcialmente actualizado de la cadena es $\boldsymbol\theta_{-j}=(\boldsymbol\theta_{1}^{(k+1)},...,\boldsymbol\theta_{j-1}^{(k+1)},\boldsymbol\theta_{j+1}^{(k)},...,\boldsymbol\theta_{q}^{(k)})'$, para la $k+1$-ésima iteración de la cadena, definimos el cociente de importancia $w(\boldsymbol\theta)=\pi(\boldsymbol\theta)/q(\boldsymbol\theta)$ y aceptamos la muestra $\boldsymbol\theta_{j}^{'}$, generada por $q(\boldsymbol\theta_{j}|\boldsymbol\theta_{-j})$, con una probabilidad $\alpha(\boldsymbol\theta,\boldsymbol\theta^{'})=\min \{1,w(\boldsymbol\theta^{'})/w(\boldsymbol\theta)\}$, donde $\boldsymbol\theta^{'}=(\boldsymbol\theta_{1}^{k+1},...,\boldsymbol\theta_{j-1}^{(k+1)},\boldsymbol\theta_{j}^{'},\boldsymbol\theta_{j+1}^{(k)},...,\boldsymbol\theta_{q}^{(k)})'$ y $\boldsymbol\theta=(\boldsymbol\theta_{1}^{k+1},...,\boldsymbol\theta_{j-1}^{(k+1)},\boldsymbol\theta_{j}^{(k)},\boldsymbol\theta_{j+1}^{(k)},...,\boldsymbol\theta_{q}^{(k)})'$, en otro caso $\boldsymbol\theta_{j}^{(k+1)}=\boldsymbol\theta_{j}^{(k)}$. Los pesos $w(\boldsymbol\theta)$ en la probabilidad de aceptación son los mismos que empleamos para aproximar integrales mediante el muestreo por importancia, previamente discutido, así que las consideraciones presentadas para la distribución instrumental $q$ tienen el mismo significado en este esquema. El componente aleatorio de aceptación garantiza que la cadena tenga a $\pi$ como distribución invariante. 

Los esquemas de muestreo que describimos previamente representan dos alternativas fle\-\-xibles para implementar procedimientos Bayesianos de inferencia. Para problemas donde no puedan ser empleados de manera directa, se pueden definir diferentes combinaciones de éstos o de algunas generalizaciones, en diferentes etapas y bajo ciertas restricciones. Esta combinaciones dan origen a lo que se conoce como métodos híbrido de MCCM \citep{Tierney_MCMC}. 

Por otro lado, éstos esquemas de muestreo están diseñados para generar muestras o una cadena de Markov, de una distribución $\pi$ definida sobre un espacio de dimensión fija. En la siguiente sección describimos un método de muestreo diseñado medidas de probabilidad $\pi$ definidas sobre un espacio de dimensiones cambiantes, en cuyo caso se generan muestras de medidas de probabilidad degeneradas en subespacios del espacio general de interés.  

\section{MCCM con Salto Reversible}

En algunos problemas como mezcla o selección Bayesiana de modelos (vea el capítulo 3), la distribución de interés $\pi $ está definida en un espacio parametral \textit{general}, denotado por $\boldsymbol\Theta $, formado por la unión de los diferentes subespacios parametrales asociados a cada modelo considerado en la mezcla o selección. A su vez los modelos son indexados por un conjunto índice $I$ (finito o numerable). De esta forma el espacio parametral está representado de la forma
\begin{equation}
\boldsymbol \Theta =\bigcup_{m\in I}\{m\}\times \boldsymbol\Theta_{m},  
\label{espacio}
\end{equation}
donde $\boldsymbol\Theta _{m}$ $\subset \Re ^{n(m)}$ es el espacio parametral asociado el modelo indexado por $m$, y $n(m)\geq 1$ es su correspondiente dimensión. En este caso, es natural
pensar en una forma jerarquizada de la distribución de interés de la forma 
\begin{equation}
\pi (m,\boldsymbol\theta _{m}|\boldsymbol y)=p(m|\boldsymbol y)\pi (%
\boldsymbol\theta _{m}|m,\boldsymbol y),  
\label{target}
\end{equation}
donde $p(m|\boldsymbol y)$ es la probabilidad final del modelo $m\in I$; $\boldsymbol\theta _{m}\in \boldsymbol\Theta _{m}$ es el parámetro asociado al modelo $m$; y $\pi (\boldsymbol\theta _{m}|m,\boldsymbol y)$ es la densidad final de $\boldsymbol\theta _{m}$ dado el modelo $m$ y la muestra $\boldsymbol y$.

Para implementar el muestreo por MCCM sobre $\boldsymbol{\Theta }$\textbf{,} se necesita definir una estrategia de saltos entre los diferentes subespacios de manera que (\ref{target}) sea la distribución invariante de la cadena $\{(m^{(k)},\boldsymbol\theta _{m^{(k)}}^{(k)})\}_{k\geq 1}$.

\citet{Green_MCMCRJ} propuso una metodología para muestrear sobre distribuciones de la forma (\ref{target}), que básicamente es una extensión del algoritmo de M-H, en el sentido que ambos construyen cadenas de Markov reversibles con distribución invariante $\pi $. Sin embargo, en este algoritmo la distribución propuesta para la evolución de la cadena en el espacio de estados de los parámetros es una distribución degenerada en uno de los subespacios del espacio general (\ref{espacio}). La idea de este algoritmo consiste en generar un nuevo valor de la cadena, $(m',\boldsymbol\theta_{m'}^{'})$, condicional en el estado de la cadena en la $k$-ésima iteración, $(m^{(k)},\boldsymbol\theta _{m^{(k)}}^{(k)})$. La construcción del nuevo valor propuesto en la cadena preserva la estructura jerárquica de (\ref{target}). En primer lugar, se genera un nuevo valor índice del modelo, $m'$ mediante la distribución de transición $J(m^{(k)},m')$ entre los índices de los modelos.

La segunda etapa del muestreo, condicional en $m'$, consiste en
obtener una muestra $\boldsymbol\theta _{m'}^{'}$ de $\pi(\boldsymbol%
\theta _{m'}|m',\boldsymbol y)$. El problema en esta etapa
consiste en definir de manera adecuada el salto entre los dos subespacios $\boldsymbol{\Theta }_{m^{(k)}}$ y $\boldsymbol{\Theta }_{m'}$ de manera que el nuevo estado de la cadena, $\boldsymbol\theta _{m'}^{'}$, dependa del estado actual de la cadena, $\boldsymbol\theta _{m^{(k)}}^{(k)}$. La idea de Green consiste en definir un emparejamiento de las dimensiones de ambos espacios y suponer que existe una biyección entre ellos, que puede definirse a través de la incorporación de variables auxiliares de la forma  $g_{m^{(k)},m'}:$ $\boldsymbol{\Theta }_{m^{(k)}}\times \boldsymbol U_{m^{(k)},m'}\rightarrow \boldsymbol{\Theta }_{m'}\times \boldsymbol U_{m',m^{(k)}}$ que mapea entre los espacios emparejados expandidos, donde $\boldsymbol U_{m^{(k)},m'}$ y $\boldsymbol U_{m',m^{(k)}}$ denotan variables auxiliares definidas de manera que $\dim (\boldsymbol{\Theta }_{m^{(k)}}\times \boldsymbol U_{m^{(k)},m'})=\dim (\boldsymbol{\Theta }_{m'}\times \boldsymbol U_{m',m^{(k)}})$.

Supongamos que el estado propuesto en la iteración $k+1$ es $(m',\boldsymbol\theta_{m'}^{'})$, la probabilidad de aceptación de estos valores como el nuevo estado de la cadena está dada por 
\begin{equation}
\alpha _{m^{(k)},m'} ( \boldsymbol\theta_{m^{(k)}}^{(k)},\boldsymbol
\theta _{m'}^{'}) =\min \left(1,r_{m^{(k)},m'}(\boldsymbol\theta _{m^{(k)}}^{(k)},\boldsymbol\theta _{m'}^{'})\right) ,  
\label{prob}
\end{equation}
donde 
\begin{eqnarray}
r_{m^{(k)},m'}(\boldsymbol\theta _{m^{(k)}}^{(k)},\boldsymbol\theta
_{m'}^{'}) &	=	& \frac{p(m') \pi(\boldsymbol\theta_{m'}^{'}|m') J(m',m^{(k)})}{p(m^{(k)}) \pi (\boldsymbol\theta_{m^{(k)}}^{(k)}|m^{(k)}) J(m^{(k)},m')} \nonumber \\
	&	&	\times \frac{q_{m',m^{(k)}} (\boldsymbol\theta_{m'}^{'},\boldsymbol u_{m',m^{(k)}})} {q_{m^{(k)},m'} (\boldsymbol\theta_{m^{(k)}}^{(k)},\boldsymbol u_{m^{(k)},m'})} \\
	&	&	\times \mathcal{J}_{g_{m^{(k)},m'}}, \nonumber
\end{eqnarray}
con 
\begin{equation}
\mathcal{J}_{g_{m^{(k)},m'}}=\left| \det 
	\frac{\partial g_{m^{(k)},m'}(\boldsymbol \theta_{m^{(k)}},\boldsymbol u_{m^{(k)},m'}) }{\partial \boldsymbol\theta_{m^{(k)}} \boldsymbol u_{m^{(k)},m'}}\right|,  
\label{Jacobiano}
\end{equation}
así, con probabilidad $\alpha _{m^{(k)},m'} (\boldsymbol\theta_{m^{(k)}}^{(k)},\boldsymbol
\theta _{m'}^{'})$ aceptamos los valores propuestos como el nuevo estado de la cadena en la iteración $k+1$. 

\citet{Green_MCMCRJ} brindan una descripción detallada de este algoritmo de muestreo. Este algoritmo esta diseñado para utilizar la información del estado actual en la propuesta de movimiento en cada iteración, aunque podemos utilizar ciertas modificaciones para simplificar la generación de estados propuestos. De acuerdo con \citet{Godsill_MCMCModelUncertainty} es posible proponer nuevos posibles valores de espacio parametral de cada modelo de manera independiente, preservando que la distribución (\ref{target}) sea la distribución invariante de la cadena. En este caso no es necesario realizar un emparejamiento de los espacios parametrales de cada modelo y de esta manera el Jacobiano (\ref{Jacobiano}) sería eliminado de la probabilidad de aceptación del movimiento. Esta es una alternativa simple, sin embargo la convergencia de la cadena podría ser más lenta, de acuerdo a la naturaleza y estructura de los modelos, como es apuntado por \citet{Godsill_MCMCModelUncertainty}.

En algunos problemas específicos las distribuciones finales de cada uno de los mo\-\-delos, i.e. $\pi(\boldsymbol\theta_{m}|m,\boldsymbol y)$ para cada $m$, son conocidas de ma\-\-nera cerrada. En este caso, si podemos generar muestras directamente de las distribuciones $\pi (\boldsymbol\theta_{m'}|m',\boldsymbol y)$, eliminamos la necesidad de emparejar los espacios parametrales entre los modelos y simplificamos la expresión de la probabilidad de aceptación de movimiento propuesto, como 
\begin{equation}
\alpha _{m^{(k)},m'}(\boldsymbol\theta _{m^{(k)}}^{(k)},\boldsymbol\theta
_{m'}^{'})=\left( 1,\frac{p(m')p(\boldsymbol %
y|m')J(m',m^{(k)})}{p(m^{(k)}) p(\boldsymbol %
y|m^{(k)})J(m^{(k)},m')}\right) ,
\label{mccmsr.simpl} 
\end{equation}
donde $p(\boldsymbol y|m)=\int $ $p(\boldsymbol y|\boldsymbol\theta _{m},m)$ 
$\pi (\boldsymbol\theta _{m}|m)d\boldsymbol\theta _{m}$ denota la \textit{%
verosimilitud integrada} del modelo $m$.

Mediante el MCCMSR, podemos ahorrarnos el trabajo de muestrear paralelamente en cada subespacio, y obtener una muestra que estará concentrada en los espacios que tenga una mayor probabilidad final, sin la necesidad de evaluar explícitamente éstas. 

\section{Consideraciones Generales}

Existen diferentes consideraciones que se deben tomar en cuenta al momento de implementar algún algoritmo de muestreo mediante MCCM. Supongamos que $\{\boldsymbol\theta^{(k)}\}_{k\geq 1}$ es una trayectoria de una cadena de Markov con distribución invariante $\pi$. La primera consideración sobre el uso de MCCM es determinar cuándo la cadena de la cual estamos simulando entra en su fase de equilibrio. Esta no es una tarea simple, y empíricamente es difícil asegurar este comportamiento, sin embargo existen diferentes método para su monitoreo, como por ejemplo, graficar la trayectoria o traza de la cadena y los promedios actualizados de cada uno de sus componentes. En la práctica es usual definir un periodo inicial de longitud considerable de manera que la cadena presumiblemente entre en su fase de equilibrio, para tratar de garantizar que la cadena no se afecte por el valor inicial de la cadena. Por otro lado, suponiendo que la cadena se encuentra dentro de su fase de equilibrio, es evidente que debido a la estructura de Markov los datos muestreados no son independientes. Para reducir este efecto podemos obtener submuestras espaciadas de la trayectoria de la cadena simulada, de manera que la autocorrelación entre los datos no sea significativa. La longitud del submuestreo es arbitraria y se determina a partir de un análisis exploratorio de la trayectoria de la cadena. Alternativamente, para garantizar una muestra independiente de $\pi$, es posible generar un gran número de cadenas de manera simultánea y conservar los valores observados de cada cadena después de un periodo o longitud adecuada, de manera que la cadena este en su fase de equilibrio. Esta alternativa es poco eficiente ya que implica un costo computacional demasiado elevado durante su implementación. 
  
Para el caso de MCCM con salto reversible, la verificación de convergencia de la cadena es aún más complicada. En principio, es posible monitorear la convergencia de la cadena para el movimiento entre modelos o en aquellas cantidades o parámetros comunes entre ellos. Pero dado que las visitas de los modelos son aleatorias, el número de muestras dentro de cada modelos es aleatorio y para algunos de los modelos las cantidades observadas no son suficientes para monitorear la convergencia de la cadena dentro de esos modelos en particular.

En la siguiente sección realizaremos una breve descripción acerca de las series de tiempo. Describiremos algunos de los modelos más usados para el análisis y predicción de series de tiempo, así como algunos métodos de inferencia y predicción Bayesiana. 

%	----------------------------
%		Modelos de regresión
%	----------------------------
\part{Modelos de Regresión}

%	----------------------------
%		Ch1. Fundamentos de los Modelos de Regresión
%	----------------------------
\chapter{Fundamentos de los Modelos de Regresión}
	
%=======================================================
%		Bibliografía
%=======================================================
\bibliographystyle{apalike}
\bibliography{References_JCMO}

\end{document}
%
%	--	FIN --